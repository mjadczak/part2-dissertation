\chapter{Preparation}\label{ch:prep}

\section{The Elixir programming language (a primer)}\label{sec:prep:elixir}

\todo{Not sure if this is too verbose. Perhaps move this to an appendix. Revisit this depending on how much code there is in the final paper.}

Elixir is a dynamically-typed, functional language which runs on the Erlang VM, which has underpinned mission-critical low-latency, fault-tolerant distributed systems for decades \cite{scalability_erlang_otp}.

This section aims to give a quick primer to the semantics and syntax of Elixir, in order to aid the readability of code snippets in this document.
The reader is encouraged to explore the language more deeply using the many available resources <references>.

\subsection{Data types}
Elixir has several basic data types: integers, floats, booleans, atoms, strings, lists, maps and tuples.
It also has some auxiliary types used in the concurrency and messaging model.
The only important one of those is the pid type, which uniquely identifies processes in a cluster, allowing us to send them messages.

Integers follow the usual syntax, and are arbitrarily sized---this is similar to how e.g.\ a \verb|BigInteger| object in Java would behave.

Floating point numbers are always 64-bit double precision floats.

Atoms, denoted for example \exs{:foo}, \exs{:foo_bar}, \exs{:"arbitrary @ string"} are essentially indexed strings---they allow for a convenient representation in code but are turned into integers (indices into an atom table) at code load-time, allowing for very fast comparisons.
As such, they are very heavily used for things like map keys, constants and places where other languages might use something like an enumeration type.

Booleans, the usual \exs{true} and \exs{false}, are actually just the atoms \exs{:true} and \exs{:false}, though the keywords can be used plain for convenience.

The value \exs{nil} is a similar case---the keywords simply denotes the atom \exs{:nil}.
This value has no special behaviour in the language, but is used by convention to denote an empty result or other missing data.

Tuples are an essential part of the language and are used extensively.
They are denoted with curly brackets, as \exs{{1, :second, "third"}} and can have arbitrary length.

As Elixir is a functional language, lists are a fundamental datatype for data processing.
They are denoted with the conventional syntax \exs{[1, 2, 3]}.
Existing lists can be consed onto: \exs{[1, 2 | tail]}.
The tail is not type-checked in this process, and it is technically possible to construct improper lists (\exs{[1 | 2
]}) but this is not generally done.
The elements of a list do not have to all be the same type.

A common construct in the Erlang / Elixir ecosystem is a keyword list---a list of two-tuples with an atom (the key) as the first element, and an arbitrary value as the second element: \exs{[{:option_1, 4}, {:option_2, "foo"}]}.
Functions with many optional arguments will typically take such a keyword list as the last argument.
In fact, this construct is so common that Elixir provides syntax sugar for it---the previous example can be written as \exs{[option_1: 4, option_2: "foo"]}.

Maps are values which store key-value data.
Neither all keys nor all values need be the same type.
A map is denoted as
\begin{minted}{elixir}
%{
  12 => "value",
  "key" => :value,
  :another_key => 67
}	
\end{minted}
however there is a convenience syntax for maps with only atom keys:
\begin{minted}{elixir}
%{
  key_1: 5,
  key_2: 12
}	
\end{minted}

String in Elixir are denoted with a double quote: \exs{"Hello World"}.
They are represented as UTF-8 encoded binaries.
Binaries are an Erlang VM-specific data type which can store arbitrary binary data and allow powerful transformations at the bit and byte level.
Strings in Elixir support interpolation of arbitrary expressions: \exs{"Hello #{name}"} will evaluate to \exs{"Hello Bob"} if \exs{name} is bound to \exs{"Bob"} in the current context.

There are also single-quoted strings, which are mostly used to interface with Erlang code.
In Erlang, strings are represented as lists of integers, with the integer representing the ASCII-code of each character.

\subsection{Modules and functions}

Code in Elixir is organised in units called modules.
An example module is given below.

\begin{minted}{elixir}
defmodule HelloWorld do
  def say_hello(name \\ "World") do
    IO.puts "Hello #{get_name(name)}!"
  end
  
  defp get_name(name), do: name
end	
\end{minted}

\todo{explain syntax features.}

\subsection{Pattern matching}

\subsection{Function pipelining}

\section{The Dataflow Model explained}\label{sec:prep:dataflow}

As mentioned in \cref{sec:intro:motivation}, the main feature of the Dataflow model is flexibility.
It aims to create a unified model of data processing which can encompass other processes, placing them in a well-defined structure and allowing the efficient execution of such systems.

It does this by separating the logical notion of data processing from the underlying physical implementation.
An abstract data processing model is defined which covers existing modes of processing by allowing the orthogonal specification of \textbf{what} results are being computed, \textbf{where} in event time they are being computed, \textbf{when} in processing time they are materialised, and \textbf{how} earlier results relate to later refinements \cite[p.~1793]{Akidau:2015}.

This standard model can then be implemented and realised by many different runners, whether natively batch, streaming or hybrid, each with its own set of advantages and disadvantages. 
This flexible approach allows for the tuning of execution technology to business requirements while working with a consistent, expressive theoretical model of the processing itself.

The key takeaway is that this model tries to make as few assumptions as possible on the data inputs, outputs and processes, instead providing simple primitives from which complex system can be constructed.

What follows is a descriptions of the primitive concepts used in the model to more precisely specify all of these properties.

\subsection{``The What'': Transforms and pipelines}

In the Dataflow model, a \emph{pipeline} is an entire data processing flow.
It may incorporate many different \emph{transforms} which pass data between each other, processing and transforming it as it flows through them.
One can think of pipelines as DAGs\footnote{Directed Acyclic Graphs}, with nodes representing transforms and edges representing the data flowing between them.

\todo{Diagrams of pipelines}

The graph does not have to be connected---it is valid for there to be multiple disjoint data processing paths.
A given transform may produce multiple outputs and it may take in multiple inputs.
It can also produce no outputs, instead causing a side effect such as writing to the network, database or file system.
Transforms can also have no inputs in the graph, instead receiving output from an external source or generating it.
These transforms are called root transforms.

A key advantage of expressing transforms in terms of data flowing is the natural adaptation of the system to unbounded data.
When we say unbounded data, we mean data which may not necessarily have an end.
An example of this may be a stream of events from a website, or the Twitter firehose.
It may help to think of unbounded data as ``streaming'' data, but ``streaming'' and ''batch'' imply execution strategies as opposed to data properties.
Further, in the Dataflow Model it is more convenient to think of all data as being streamed, whether that is a true infinite stream, or merely data being streamed out of a file.

All transforms can be represented fundamentally using only two primitives: \verb|ParDo| and \verb|GroupByKey|\footnotemark[2].

\footnotetext[2]{
Even though these transforms are the only primitives mentioned in the original description of the Model \cite[ยง2.1]{Akidau:2015}, real implementation will likely special-case several other transforms such as \texttt{AssignTimestamps} or \texttt{Window} and make them primitives for architectural or performance reasons.
}

\todo{diagrams of the transformations performed by these}

\verb|ParDo| represents a flat-map operation---it transforms a data element into zero or more data elements.
The actual computation performed can be arbitrary---the only restriction is that the output depends only on the single element being processed.
This restriction means that the \verb|ParDo| step is ``embarrassingly parallelisable''.
It also translates naturally to operating on unbounded data, as we can emit the output in real-time.

\verb|GroupByKey|, on the other hand, simply groups all key-value pairs into elements with a single key and a collection of elements.
It needs to collect \emph{all} of the data for a particular key before being able to emit its output.
An issue arises when dealing with unbounded data---how do we know when we've received all of the input for a particular key?

To deal with this problem, the Dataflow Model introduces a concept of windowing.

\todo{somewhere we need an explanation of elements' structure as they flow through the pipeline.}

\subsection{``The Where'': Windowing}

Windowing simply means the additional grouping of elements according to their timestamps in a way which allows us to emit a particular chunk of elements when we know that we have all the data for it.
In the Dataflow Model, we call such a chunk of data belonging to a particular window a \emph{pane}.
Before diving into the details, let us take a brief detour into what ``time'' means in this context.

\subsubsection{Time domains}
We distinguish between two inherent time domains when considering our data as events in time: \emph{event time} and \emph{processing time}.

Event time is the time at which the event actually occurred and is an inherent property of the element itself.
We also refer to this value as the \emph{timestamp} of an element.
Each element in a pipeline must have a timestamp.
We ensure that we have two ``special'' values available, the \emph{minimum} and \emph{maximum timestamps}.
Implementations may make these actual special values, or just use the maximal and minimal values of the underlying representation.

Where there is no inherent time associated with an element (for example it is an un-dated record in a file) or we don't know the timestamp yet (we must process the record before we can extract the timestamp), the element is assigned the minimum timestamp.
The logic of this will become apparent in the following sections.

Processing time is the property of an executing pipeline.
It measures real-world time elapsing as the computation progresses.
The processing time of a particular element at a particular transform is simply the system clock time at which it was seen or processed by that transform.
The Model makes no assumptions about the synchronisation of this time across multiple distributed transforms.

\subsubsection{Types of windows}

\todo{describe global, fixed, sliding and session windowing}

\todo{diagrams showing visually how different windows work}

\subsubsection{Windowing in the Dataflow Model}
The key insight in the Dataflow Model which allows for very flexible windowing is the generalisation of what a window is.

We define the concept of a \emph{windowing function}, which is actually a pair of two functions: \texttt{assign} and \texttt{merge}.

\texttt{assign} takes the timestamp of the element (and optionally the element's data) and outputs a set of windows to which it should be assigned.
This may be a singleton set when windows are non-overlapping, or it could be many windows in the case of e.g.\ sliding windows.
It's important to note that if the element is assigned to multiple windows, we are essentially duplicating that element into each of those windows (since all computation is implicitly grouped by the window).

\texttt{merge} takes a set of windows and optionally merges some or all of them into a new set of windows.
This is used in windowing strategies such as the session windowing mentioned earlier---this function will detect smaller windows near each other and merge them into longer sessions.
Not all windowing functions take advantage of this, and in fact the common strategies apart from session windows are all non-merging.

\todo{diagram / example of how values are transformed with windowing and grouping}

\todo{show an example of the two functions being used to effect session-based windowing}

\subsection{``The When'': Watermarks and triggers}

In the previous section, we have defined a way to group elements together into windows.
We have also asserted that we can ``emit a particular chunk of elements when we know that we have all the data for it''.
How can we achieve this in light of the fact that data may appear out of order?

The true answer is that we cannot---if we allow the possibility of unpredictably late data in our pipeline, then we cannot ever truly be certain that we can emit a complete pane of data.
What we can do, however, is introduce a model which allows us to specify the desired behaviour in a useful and flexible manner.
Let us first define the concept of \emph{watermarks}.

\subsubsection{Watermarks}

A \emph{watermark} is a lower bound, in event time, on the timestamps of values which have been processed by the pipeline (or a particular transform therein).
For example, if at a particular instant the watermark is time $T$, then we are being promised that all events with timestamps earlier than $T$ have been processed.
A graph of watermark progression against processing time (such as the one in <TODO ref here>) is a useful way to visualise the progress of a pipeline.
Further, this concept allows us to construct a powerful model of lateness in the system.
This is explored further in \cref{sec:prep:dataflow_extra:lateness}.

\todo{diagram of elements being processed in time vs.\ a watermark}

The reader may at this point be asking, ``where does the watermark come from?''
Watermarks in the Dataflow Model are heuristics generated by sources of data, or by user logic in the pipeline.
Their exact behaviour is touched upon in \cref{sec:prep:dataflow_extra:lateness,sec:prep:todo}.

\subsubsection{Triggering model}

An initial suggestion may be to simply use the watermark as an indication of when a window is finished and is safe to emit; that is, to emit once the watermark has passed the end of the window.
There are two major shortcomings with this strategy:
\begin{itemize}
	\item The watermark may be \textbf{too fast}---we may have decided that all elements for a given window have been seen and emit a pane, only to later on receive a late element which should have been in the window.
	It is unclear what to do with the element in this case.
	
	\item The watermark may be \textbf{too slow}---the heuristic can be too conservative, and even when we have access to the true watermark and not merely a heuristic, a single slow datum can hold up processing for the entire pipeline.
	Therefore using watermarks as the sole signal for emitting data can lead to high latency which may be highly undesirable in some scenarios.
\end{itemize} 

To solve these problems, we define a more complex triggering model.

Firstly, we explicitly allow for data in a particular window to be emitted multiple times.
We call each such emission a pane, and every pane is marked as either early, on time or late.
Secondly, we define a trigger as simply some arbitrary code which has access to time information of elements passing through the transform, as well as watermark and other timing information.
In practice, we define a tree of triggers allowing us to easily achieve desired results (this is explored further in <todo ref>). 

In this manner we can allow for many different behaviours---for example, we may want to receive approximate results (early panes) every ten seconds (in processing time) until the watermark passes the end of the window, at which point an on time pane is emitted.
Any further late data arriving after the watermark can trigger the emission of a late pane.
This, and myriad other combinations, are all possible with this model.
A key strength is that it decouples the marking of data as early/tentative, on time/definitive, or late/corrective, from actually processing the different panes in different ways.

\todo{maybe include a description of a particular use case and which options one would choose for it?}

\todo{one sentence conclusion of triggering / windowing}

\subsection{``The How'': Refinements and retractions}

While the concepts defined so far allow us to specify when panes should be emitted, we haven't yet settled on what their contents should be, exactly.
We do this by selecting from one of the three refinement modes for a particular \verb|GroupByKey| transform:
\begin{itemize}
	\item The \textbf{discarding} mode simply discards the contents of the window after emitting a pane, such that any new pane emitted will contain only data received since the last emission. There is no relation between the contents of any two panes.
	\item The \textbf{accumulating} mode retains the contents of the window after emitting a pane. A later pane will include all of the data from the initial pane, in addition to any data which has arrived since.
	\item The \textbf{accumulating \& retracting} mode emits retractions to previous panes emitted, as well as following the semantics of the accumulating mode. This means that downstream transforms will first receive notification that the previous pane is now invalid (along with the contents of that pane again, to aid in undoing its results), and after this they will receive the contents of the new pane (which includes the contents of the initial pane, now refined with additional data). 
\end{itemize}

\todo{example of when to use one or the other?}

While the Model broadly defines the concept of retractions and how they may work at a high level, their implementation remains an unsolved problem and is actively being worked on by the Beam project.
The main difficulty lies in the explosion of cached state which seems to be needed in order to allow the undoing of certain operations.
Therefore, the implementation and precise specification of the behaviour of retractions are out of the scope of this paper, and will not be mentioned again.

With the core primitives of the Dataflow Model defined, we will now briefly look at the work done in extending it into a production-ready implementation by the Apache Beam project before considering the additional theory and concepts which must be defined in order to specify the behaviour of such an implementation.

\section{A survey of the Apache Beam project}\label{sec:prep:beam}

The Apache Beam project is a multifaceted undertaking, aiming to build an entire ecosystem around the Model.
The components of the project can be coarsely categorised into SDKs and \emph{runners}.

SDKs provide the user interface to the processing engine.
They are language-specific libraries which allow users to specify the computations to be performed using the tools of their native language, without having to worry about how the computation will be executed.
Currently there is a main, fully-featured Java SDK which serves as the \emph{de~facto} standard implementation.
There is also work being done on a Python version of the SDK.

\todo{``as of the time of writing''?}

SDKs produce descriptions of data processing pipelines in a standard format, which any runner can consume and execute.
This allows the flexibility of being able to select and tune the execution engine orthogonally to the computation itself in accordance with the needs of each user, while keeping a lot of this complexity out of the user-facing data processing model.

As mentioned in \cref{sec:intro:previous}, there is a commercial runner available through Google's Cloud Dataflow product \cite{CloudDataflow} which is a fully-managed, auto-scaling service to execute Beam pipelines.
There are also many runners which utilise other open-source data processing projects, helping to leverage existing resources or run large workloads on-premises rather than in the cloud.

Not all runners support all of the model's features, and the range of support is summarised in \emph{capability matrices}.
A full copy of the official capability matrix can be found in <appendix ref>.

\todo{larger question: is it worth referencing JIRA issues or mailing list discussions? Is it ok to just assert that certain things are the case or are being worked on?}

\todo{include capability matrices? Maybe construct one for this implementation? (https://beam.apache.org/documentation/runners/capability-matrix/)}

A crucial part of the Beam project is the \verb|DirectRunner|---the local on-machine runner provided with the Java SDK.
Its aims are not to necessarily provide a high-performance implementation, but rather to act as the standard implementation of behaviour for other runners to follow.
It is used by users to test out pipelines before sending them to a large-scale runner for production-scale execution.
In fact, much of its functionality is in a separate ``Core'' module, which contains code for other runners to directly use.

This runner is crucial for this project, as it is the result of many man-years of work to come up with a practical implementation of the Dataflow Model while keeping with its definition.
It is the runner whose main focus is model correctness, and as such it is the most appropriate to take as a reference implementation for a project which aims to explore the Dataflow Model, such as this one.

A side-effect of the way the project has developed, however, is that a large amount of further development to the theory of the model has occurred implicitly while developing this runner.
This happened in place of a formal definition of many concepts, a large amount of which were sketched out in proposals for the project and then iterated upon in code.

Therefore, a huge part of this project was the extraction and reverse-engineering of these concepts from the Java codebase into a more formal specification.
This was further necessitated by the move from an OOP to a functional approach, requiring each concept to be reduced to what it is in the abstract rather than just transferring the class hierarchy to a new language.



\section{The Dataflow Model extended: what the paper doesn't tell you}\label{sec:prep:dataflow_extra}

\todo{Maybe this section should all go into implementation? It's the work done while trawling through the Beam code, concurrently with writing my own.}

\subsection{Managing time}

\todo{Actually, the previous section talked about time domains etc. Details of a separate process managing time per transform should go into the implementation.}

\subsection{Windows, panes, triggers, timers and holds}

\todo{Arguably, windows, panes and triggers were already introduced in enough detail. Describe timers and watermark holds more thoroughly. This may end up getting merged with the next subsection.}

\subsection{``Lateness'' and its semantics}\label{sec:prep:dataflow_extra:lateness}

\todo{Write up existing notes on lateness.}

\subsection{Bundling values: the fine line between streaming and micro-batch}

\todo{Short section on the different approaches which can be taken by runners. How the Java implementation uses bundled to group elements. How the Elixir implementation considers any data a continuous stream, but how the data is actually processed in chunks. Refer to (hopefully) more robust timing diagram in the implementation section.}

\section{Implementation approach: from objects to actors}\label{sec:prep:implementation}

\todo{consider moving this to implementation.}

\subsection{Making state explicit and exposing hidden assumptions}

\todo{discuss moving from a stateful approach where things can just grab state out of thin air and mutate it, into an explicit approach of functions taking state and returning a modified version of the state. Also expand on how this forces greater focus on what a particular function actually does as it needs to explicitly receive things it needs, and explicitly return things it touches/modifies.}

\subsection{Managing concurrency with OTP and GenServers}

\todo{Introduce the concept of OTP, GenServers and how they provide robust, serialised concurrency. Make clear how building on top of these core libraries is advantageous and appropriate. Mention how clear semantics enable concurrency design.}

\subsection{The GenStage library}

\todo{Introduce concept of GenStage and how it builds on GenServer. Mention how its operation is a great fit for the type of computation being done in Dataflow.}

\section{Software Engineering approach}\label{sec:prep:softeng}

\subsection{Changing goals and assumptions}

A key point to note is that the project involved building and implementing the software concurrently with actually discovering its requirements.
As mentioned in \cref{sec:prep:dataflow_extra}, a huge amount of key concepts and logic had to be reverse-engineered from a massive Java codebase, or put together from discussions on mailing lists and issue trackers.

This resulted in a need for a highly adaptive and agile development process, since fundamental assumptions made initially changed many times over.

\todo{give actual examples of this happening maybe?}

The Elixir language, with its clear modular structure combined with writing simple, composable functions enabled a flexible model where key logic in the codebase could be refactored with ease when needed.

\subsection{The spiral model}

\todo{summarise spiral model, refer to previous subsection as to why it's good. Include a diagram.}

\subsection{Testing approach}

\todo{mention small, testable modules as well as end-to-end examples}

\section{Starting point}\label{sec:prep:starting}

\todo{Good knowledge of Elixir, and not much more regarding getting to know how Beam ticks inside.}

\section{Summary}\label{sec:prep:summary}