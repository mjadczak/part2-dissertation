\chapter{Preparation}\label{ch:prep}

\section{The Elixir programming language (a primer)}\label{sec:prep:elixir}

Elixir is a modern, functional language which executes on the BEAM VM, the standard VM used by Erlang.
It has many interesting and powerful features, and the reader is encouraged to make use of the many excellent resources in order to study it in depth.
This section aims only to provide a short description of the language to provide context for the implementation chapter.
Any special syntax or semantics in code examples will be explained where introduced.

\subsection{Language basics}

Elixir is a dynamically-typed, garbage collected functional language.
Values are immutable and looping behaviours are implemented through recursion, making use of tail-call optimisation, as in many other functional languages.

Elixir has the usual assortment of useful data types including arbitrary-sized integers, arbitrary-sized tuples, lists, maps (dictionaries) and strings (a special case of the ``binary'' type which can store arbitrary bytes or bitstrings).
It also has the concept of ``atoms'' denoted \exs{:foo}, which in code look like special strings, but at runtime are actually integers indexing into a global atom table, making comparisons very fast.
As such, atoms are used very often to tag tuples, for example \exs{{:ok, result}}.
These tagged tuples are very flexible and often replace the role that abstract data types in more classical functional languages.

Elixir has first-class functions and supports both defining lambdas and capturing existing functions within modules as lambdas.
All values in Elixir are automatically able to be serialised.
This includes functions and modules, which are serialised in their bytecode form, and makes it very easy to ship code over the wire, facilitating the distribution of work across a system.

Elixir also has powerful pattern-matching capabilities, available in function heads and \exs{case} expressions.
Since all values in the language can be directly expressed in the syntax (there are no user-definable types) any value can be directly matched, and values such as tuples, maps and list can be destructured and their contents matched directly.

Elixir uses modules to organise functions.
A module is simply a namespace in which functions and macros, both public and private, can be placed.
Module names are atoms themselves, and in fact we can store them pass them around, dynamically calling functions within them at runtime.

Elixir has a very powerful macro system, which allows for the arbitrary transformation of the AST at compile-time.
In fact, most of the standard language directives such as those which define modules and functions or control the flow of the program are themselves macros.

Elixir programs often adopt the paradigm of data being transformed by flowing through a pipeline of functions.
To that end, the convention in Elixir for functions which operate on data is to take the thing being transformed as the first argument to the function.
Then, we make use of the pipeline operator \exs{|>} to write things like
\begin{minted}{elixir}
"foo"
|> String.capitalize()
|> String.pad_leading(5)
# produces "  foo"
\end{minted}
which is equivalent to
\begin{minted}{elixir}
String.pad_trailing(String.capitalize("foo"), 10)		
\end{minted}
but expresses the steps in the computation much more clearly.
This convention matches very well with the Dataflow model, which also operates in terms of defining a pipeline of transformations for data to flow through.

\subsection{Concurrency model and the OTP framework}

Elixir and Erlang have a concept of processes, which are very lightweight ``threads'' managed by the VM itself and scheduled transparently across hardware threads.
The VM can support many thousands of processes with a predictable, linear performance curve, making it very suitable for writing highly concurrent systems.
Processes each have their own heap and can communicate with each other by sending messages.
Data passed in a message is copied.

OTP is a framework shipped with the standard Erlang distribution which provides a way to develop robust concurrent and distributed systems, providing powerful tools to isolate, monitor and act on failures.
It is used to organise processes into supervision trees, with certain processes being responsible for starting, supervising (monitoring for crashes and errors, potentially restarting child processes) and ending child processes.
In this way we can isolate failures and handle them explicitly, giving the programmer the tools to create robust and failure-tolerant systems.

OTP also provides powerful concurrency primitives such as state machines and generic servers which handle many common edge cases and failure modes so prevalent in distributed systems.
It also allows for the connection of multiple BEAM nodes in a cluster, allowing for the transparent sending of messages between processes on different nodes.

It is a typical pattern to use these primitives to build systems with an actor model---isolated processes each handling a particular concern, communicating by sending messages.

While only a small portion of the power of OTP is used in the implementation in this project, a full production-ready distributed implementation of the Model in Elixir would be able to take advantage of the many advanced primitives available to greatly reduce the code complexity and potentially performance when compared to the JVM implementations.
In fact, many thousands of lines of code in the Java implementation are dedicated to solving the same problems and implementing the same features that one gets ``for free'' by using the BEAM VM.

\section{The Dataflow Model explained}\label{sec:prep:dataflow}

As mentioned in \cref{sec:intro:motivation}, the main feature of the Dataflow model is flexibility.
It aims to create a unified model of data processing which can encompass other processes, placing them in a well-defined structure and allowing the efficient execution of such systems.

It does this by separating the logical notion of data processing from the underlying physical implementation.
An abstract data processing model is defined which covers existing modes of processing by allowing the orthogonal specification of \textbf{what} results are being computed, \textbf{where} in event time they are being computed, \textbf{when} in processing time they are materialised, and \textbf{how} earlier results relate to later refinements \cite[p.~1793]{Akidau:2015}.

This standard model can then be implemented and realised by many different runners, whether natively batch, streaming or hybrid, each with its own set of advantages and disadvantages. 
This flexible approach allows for the tuning of execution technology to business requirements while working with a consistent, expressive theoretical model of the processing itself.

The key takeaway is that this model tries to make as few assumptions as possible on the data inputs, outputs and processes, instead providing simple primitives from which complex system can be constructed.

What follows is a descriptions of the primitive concepts used in the model to more precisely specify all of these properties.

\subsection{``The What'': Transforms and pipelines}\label{sec:prep:dataflow:what}

In the Dataflow model, a \emph{pipeline} is an entire data processing flow.
It may incorporate many different \emph{transforms} which pass data between each other, processing and transforming it as it flows through them.
One can think of pipelines as DAGs\footnote{Directed Acyclic Graphs}, with nodes representing transforms and edges representing the data flowing between them.

\todo{Diagrams of pipelines}

The graph does not have to be connected---it is valid for there to be multiple disjoint data processing paths.
A given transform may produce multiple outputs and it may take in multiple inputs.
It can also produce no outputs, instead causing a side effect such as writing to the network, database or file system.
Transforms can also have no inputs in the graph, instead receiving output from an external source or generating it.
These transforms are called root transforms.

A key advantage of expressing transforms in terms of data flowing is the natural adaptation of the system to unbounded data.
When we say unbounded data, we mean data which may not necessarily have an end.
An example of this may be a stream of events from a website, or the Twitter firehose.
It may help to think of unbounded data as ``streaming'' data, but ``streaming'' and ''batch'' imply execution strategies as opposed to data properties.
Further, in the Dataflow Model it is more convenient to think of all data as being streamed, whether that is a true infinite stream, or merely data being streamed out of a file.

It is worth making the distinction between building the pipeline by conceptually ``connecting'' the different transforms, and actually executing the pipeline with real data.
In the first case, what we are producing is really a description of the running pipeline---we describe what the transforms should do, and what the data flowing between them will look like.
When we execute the pipeline, we need to actually materialise the transforms as some sort of structure which actually executes the processing steps declared earlier.
The data flowing through at execution-time is made up of elements which can be thought of as tuples with the data itself (often assumed to be key-value pairs), timestamps (intrinsic to the element) and any windows which have been assigned to the element (described later).

All transforms can be represented fundamentally using only two primitives: \verb|ParDo| and \verb|GroupByKey|\footnotemark[2].

\footnotetext[2]{
Even though these transforms are the only primitives mentioned in the original description of the Model \cite[ยง2.1]{Akidau:2015}, real implementation will likely special-case several other transforms such as \texttt{AssignTimestamps} or \texttt{Window} and make them primitives for architectural or performance reasons.
}

\todo{diagrams of the transformations performed by these}

\verb|ParDo| represents a flat-map operation---it transforms a data element into zero or more data elements.
The actual computation performed can be arbitrary---the only restriction is that the output depends only on the single element being processed.
This restriction means that the \verb|ParDo| step is ``embarrassingly parallelisable''.
It also translates naturally to operating on unbounded data, as we can emit the output in real-time.

\verb|GroupByKey|, on the other hand, simply groups all key-value pairs into elements with a single key and a collection of elements.
It needs to collect \emph{all} of the data for a particular key before being able to emit its output.
An issue arises when dealing with unbounded data---how do we know when we've received all of the input for a particular key?

To deal with this problem, the Dataflow Model introduces a concept of windowing.

\subsection{``The Where'': Windowing}\label{sec:prep:dataflow:where}

Windowing simply means the additional grouping of elements according to their timestamps in a way which allows us to emit a particular chunk of elements when we know that we have all the data for it.
In the Dataflow Model, we call such a chunk of data belonging to a particular window a \emph{pane}.
Before diving into the details, let us take a brief detour into what ``time'' means in this context.

\subsubsection{Time domains}
We distinguish between two inherent time domains when considering our data as events in time: \emph{event time} and \emph{processing time}.

Event time is the time at which the event actually occurred and is an inherent property of the element itself.
We also refer to this value as the \emph{timestamp} of an element.
Each element in a pipeline must have a timestamp.
We ensure that we have two ``special'' values available, the \emph{minimum} and \emph{maximum timestamps}.
Implementations may make these actual special values, or just use the maximal and minimal values of the underlying representation.

Where there is no inherent time associated with an element (for example it is an un-dated record in a file) or we don't know the timestamp yet (we must process the record before we can extract the timestamp), the element is assigned the minimum timestamp.
The logic of this will become apparent in the following sections.

Processing time is the property of an executing pipeline.
It measures real-world time elapsing as the computation progresses.
The processing time of a particular element at a particular transform is simply the system clock time at which it was seen or processed by that transform.
The Model makes no assumptions about the synchronisation of this time across multiple distributed transforms.

\subsubsection{Types of windows}

\todo{describe global, fixed, sliding and session windowing}

\todo{diagrams showing visually how different windows work}

\subsubsection{Windowing in the Dataflow Model}
The key insight in the Dataflow Model which allows for very flexible windowing is the generalisation of what a window is.

We define the concept of a \emph{windowing function}, which is actually a pair of two functions: \texttt{assign} and \texttt{merge}.

\texttt{assign} takes the timestamp of the element (and optionally the element's data) and outputs a set of windows to which it should be assigned.
This may be a singleton set when windows are non-overlapping, or it could be many windows in the case of e.g.\ sliding windows.
It's important to note that if the element is assigned to multiple windows, we are essentially duplicating that element into each of those windows (since all computation is implicitly grouped by the window).

\texttt{merge} takes a set of windows and optionally merges some or all of them into a new set of windows.
This is used in windowing strategies such as the session windowing mentioned earlier---this function will detect smaller windows near each other and merge them into longer sessions.
Not all windowing functions take advantage of this, and in fact the common strategies apart from session windows are all non-merging.

\todo{diagram / example of how values are transformed with windowing and grouping}

\todo{show an example of the two functions being used to effect session-based windowing}

\subsection{``The When'': Watermarks and triggers}\label{sec:prep:dataflow:when}

In the previous section, we have defined a way to group elements together into windows.
We have also asserted that we can ``emit a particular chunk of elements when we know that we have all the data for it''.
How can we achieve this in light of the fact that data may appear out of order?

The true answer is that we cannot---if we allow the possibility of unpredictably late data in our pipeline, then we cannot ever truly be certain that we can emit a complete pane of data.
What we can do, however, is introduce a model which allows us to specify the desired behaviour in a useful and flexible manner.
Let us first define the concept of \emph{watermarks}.

\subsubsection{Watermarks}

A \emph{watermark} is a lower bound, in event time, on the timestamps of values which have been processed by the pipeline (or a particular transform therein).
For example, if at a particular instant the watermark is time $T$, then we are being promised that all events with timestamps earlier than $T$ have been processed.
A graph of watermark progression against processing time (such as the one in <TODO ref here>) is a useful way to visualise the progress of a pipeline.
Further, this concept allows us to construct a powerful model of lateness in the system.
This is explored further in \cref{sec:prep:dataflow_extra:lateness}.

\todo{diagram of elements being processed in time vs.\ a watermark}

The reader may at this point be asking, ``where does the watermark come from?''
Watermarks in the Dataflow Model are heuristics generated by sources of data, or by user logic in the pipeline.
The exact behaviour is described in \cref{sec:impl:dataflow}.

\subsubsection{Triggering model}

An initial suggestion may be to simply use the watermark as an indication of when a window is finished and is safe to emit; that is, to emit once the watermark has passed the end of the window.
There are two major shortcomings with this strategy:
\begin{itemize}
	\item The watermark may be \textbf{too fast}---we may have decided that all elements for a given window have been seen and emit a pane, only to later on receive a late element which should have been in the window.
	It is unclear what to do with the element in this case.
	
	\item The watermark may be \textbf{too slow}---the heuristic can be too conservative, and even when we have access to the true watermark and not merely a heuristic, a single slow datum can hold up processing for the entire pipeline.
	Therefore using watermarks as the sole signal for emitting data can lead to high latency which may be highly undesirable in some scenarios.
\end{itemize} 

To solve these problems, we define a more complex triggering model.

Firstly, we explicitly allow for data in a particular window to be emitted multiple times.
We call each such emission a pane, and every pane is marked as either early, on-time or late.
Secondly, we define a trigger as simply some arbitrary code which has access to time information of elements passing through the transform, as well as watermark and other timing information.
In practice, we define a tree of triggers allowing us to easily achieve desired results (this is explored further in <todo ref>). 

In this manner we can allow for many different behaviours---for example, we may want to receive approximate results (early panes) every ten seconds (in processing time) until the watermark passes the end of the window, at which point an on-time pane is emitted.
Any further late data arriving after the watermark can trigger the emission of a late pane.
This, and myriad other combinations, are all possible with this model.
A key strength is that it decouples the marking of data as early/tentative, on-time/definitive, or late/corrective, from actually processing the different panes in different ways.

\todo{maybe include a description of a particular use case and which options one would choose for it?}

The combination of these flexible windowing and triggering models allow for a powerful, yet predictable method of grouping and emitting data as needed, even if it is sourced from a mixture of heterogeneous, unordered, infinite streams.
To provide these advantages, however, a surprising amount of extra machinery is required.
This is described in more detail in \cref{sec:impl:dataflow}.

\subsection{``The How'': Refinements and retractions}\label{sec:prep:dataflow:how}

While the concepts defined so far allow us to specify when panes should be emitted, we have not yet settled on what their contents should be, exactly.
We do this by selecting from one of the three refinement modes for a particular \verb|GroupByKey| transform:
\begin{itemize}
	\item The \textbf{discarding} mode simply discards the contents of the window after emitting a pane, such that any new pane emitted will contain only data received since the last emission. There is no relation between the contents of any two panes.
	\item The \textbf{accumulating} mode retains the contents of the window after emitting a pane. A later pane will include all of the data from the initial pane, in addition to any data which has arrived since.
	\item The \textbf{accumulating \& retracting} mode emits retractions to previous panes emitted, as well as following the semantics of the accumulating mode. This means that downstream transforms will first receive notification that the previous pane is now invalid (along with the contents of that pane again, to aid in undoing its results), and after this they will receive the contents of the new pane (which includes the contents of the initial pane, now refined with additional data). 
\end{itemize}

\todo{example of when to use one or the other?}

While the Model broadly defines the concept of retractions and how they may work at a high level, their implementation remains an unsolved problem and is actively being worked on by the Beam project.
The main difficulty lies in the explosion of cached state which seems to be needed in order to allow the undoing of certain operations.
Therefore, the implementation and precise specification of the behaviour of retractions are out of the scope of this paper, and will not be mentioned again.

With the core primitives of the Dataflow Model defined, we will now briefly look at the work done in extending it into a production-ready implementation by the Apache Beam project before considering the additional theory and concepts which must be defined in order to specify the behaviour of such an implementation.

\section{A survey of the Apache Beam project}\label{sec:prep:beam}

The Apache Beam project is a multifaceted undertaking, aiming to build an entire ecosystem around the Model.
The components of the project can be coarsely categorised into SDKs and \emph{runners}.

SDKs provide the user interface to the processing engine.
They are language-specific libraries which allow users to specify the computations to be performed using the tools of their native language, without having to worry about how the computation will be executed.
As of the time of writing, there is a main, fully-featured Java SDK which serves as the \emph{de~facto} standard implementation.
There is also work being done on a Python version of the SDK.

SDKs produce descriptions of data processing pipelines in a standard format, which any runner can consume and execute.
This allows the flexibility of being able to select and tune the execution engine orthogonally to the computation itself in accordance with the needs of each user, while keeping a lot of this complexity out of the user-facing data processing model.

As mentioned in \cref{sec:intro:previous}, there is a commercial runner available through Google's Cloud Dataflow product \cite{CloudDataflow} which is a fully-managed, auto-scaling service to execute Beam pipelines.
There are also many runners which utilise other open-source data processing projects, helping to leverage existing resources or run large workloads on-premises rather than in the cloud.

Not all runners support all of the model's features, and the range of support is summarised in \emph{capability matrices}.
A full copy of the official capability matrix can be found in <appendix ref>.

\todo{larger question: is it worth referencing JIRA issues or mailing list discussions? Is it ok to just assert that certain things are the case or are being worked on?}

\todo{include capability matrices? Maybe construct one for this implementation? (https://beam.apache.org/documentation/runners/capability-matrix/)}

A crucial part of the Beam project is the \verb|DirectRunner|---the local on-machine runner provided with the Java SDK.
Its aims are not to necessarily provide a high-performance implementation, but rather to act as the standard implementation of behaviour for other runners to follow.
It is used by users to test out pipelines before sending them to a large-scale runner for production-scale execution.
In fact, much of its functionality is in a separate ``Core'' module, which contains code for other runners to directly use.

This runner is crucial for this project, as it is the result of many man-years of work to come up with a practical implementation of the Dataflow Model while keeping with its definition.
It is the runner whose main focus is model correctness, and as such it is the most appropriate to take as a reference implementation for a project which aims to explore the Dataflow Model, such as this one.


\section{Changing goals}

A side-effect of the way the Dataflow/Beam project has developed is that a large amount of development of the theory of the model has occurred implicitly while the software was worked on.
This happened in place of a formal definition of many concepts, a large amount of which were sketched out in proposals for the project and then iterated upon in code.

Therefore, a huge part of this project was the extraction and reverse-engineering of these concepts from the Java codebase into a more approachable specification.
This was further necessitated by the move from an OOP to a functional approach, requiring each concept to be reduced to what it is in the abstract rather than just transferring the class hierarchy to a new language.

The initial project goals did not account for this---it was assumed that a reasonable implementation could be produced from the Dataflow paper \cite{Akidau:2015} using the Beam code simply as reference.


\section{Software Engineering methods}\label{sec:prep:softeng}

\subsection{The spiral model}

The highly fluid goals of the project resulted in a need for a highly adaptive and agile development process, since fundamental assumptions made initially changed many times over.
Often core modules had to be re\"implemented multiple times over, as the model being extracted from the existing codebase grew more complete.

The Elixir language, with its clear modular structure combined with writing simple, composable functions enabled a flexible model where key logic in the codebase could be refactored with ease when needed.
While its explicit nature meant that modules did actually have to be rewritten instead of functionality piled on top as would be possible in a mutable OOP language, this actually resulted in cleaner and more maintainable code with a clear focus.

Owing to this, the spiral model <ref?> of development was adopted, with core modules prototyped, integrated, and then refactored, extended or rewritten as other parts of the system demanded it.
Overall three or four iterations of prototyping and evaluating were performed, going all the way from a basic proof-of-concept to a working system implementing most key features of the Model.

\todo{summarise spiral model itself? Include a diagram.}

\subsection{Testing approach}

\todo{mention small, testable modules as well as end-to-end examples}

\section{Starting point}\label{sec:prep:starting}

The project was started ``from scratch''---all core code in the implementation was written specifically for this project, during the timeframe of the project itself.
The author had a good working knowledge of Elixir, having had implemented several project in it before, but had no experience in working with Dataflow or Beam.

\section{Summary}\label{sec:prep:summary}

\todo{is this needed?}