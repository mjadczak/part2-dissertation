\chapter{Preparation}\label{ch:prep}

The project builds upon an existing foundation of tools and concepts.
This chapter introduces the Elixir language used in its implementation, as well as providing a summary of concepts described in the original Dataflow paper.
The Software Engineering approach employed is also described.

\section{The Elixir programming language (a primer)}\label{sec:prep:elixir}

Elixir is a modern language which executes on the BEAM VM.
This section provides a short description of the language to give context for \cref{ch:impl}.

\subsection{Language basics}\label{sec:prep:elixir:basics}

Elixir is a dynamically-typed, garbage-collected, tail-call-optimised functional language with immutable values.
It has the usual assortment of useful data types including arbitrary-sized integers, tuples, lists, maps and \emph{binaries} (which specialise to strings)~\cite[p.~23]{Thomas:2016}.

It also has \emph{atoms}, denoted \exs{:foo}.
Atoms are constant literals, with their name as their value, and can be compared very efficiently.

As such, atoms are often used to tag tuples, for example \exs{{:ok, result}}.
These tagged tuples often replace the role of abstract data types in classical functional languages.
Atoms are also often used as keys in maps~\cite[p.~29]{Thomas:2016}.

Elixir uses \emph{modules} to organise functions.
A module is a namespace in which functions and macros, both public and private, can be placed~\cite[p.~58]{Thomas:2016}.
Elixir has first-class functions and supports both creating lambdas and capturing existing functions within modules as lambdas~\cite[p.~37]{Thomas:2016}.

All values in Elixir can be serialised in a standard way.
This includes functions and modules, which are serialised in their bytecode form.
That property makes it easy to send code over the wire, facilitating the distribution of work across a system.

Elixir has powerful pattern-matching capabilities, available in function heads and \exs{case} expressions~\cite[p.~13]{Thomas:2016}.
Since all values in the language can be directly expressed syntactically (there are no user-definable types), values such as tuples, maps and lists can be destructured and their contents matched directly.
\Cref{lst:prep:pattern-matching-example} shows an example combining atoms and pattern matching.

\begin{listing}[h]
	\caption[An example Elixir module, showcasing pattern-matching.]{Elixir uses modules to organise code. Functions can have multiple heads and use pattern-matching on their arguments in order to destructure them.}
	\label{lst:prep:pattern-matching-example}
	\begin{minted}{elixir}
defmodule WidgetExample do

  @doc """
  The function `get_widget_name!` takes a widget, or a list of widgets,
  and returns its name.
  In the case of a list the name of the first widget is returned.

  Raises an error if the operation fails. 
  """
  # The optional @spec line documents the types accepted and returned
  # by a function, as well as improving the results of static analysis tools. 
  @spec get_widget_name!([Widget.t] | Widget.t) :: String.t
  # This function head is only executed
  # when the argument is a list with at least one element.
  def get_widget_name!([widget | _]) do
    get_widget_name!(widget)
  end
  # Maps are denoted `%{key => value}`.
  # `%Widget{}` is a special type of map, with an associated module.
  # It is called a struct.
  # `%{key: value}` is a shorthand when using atoms as map keys.
  # It is equivalent to writing `%{:key => value}`.  
  def get_widget_name!(%Widget{id: widget_id}) do
    case WidgetManager.get_name(widget_id) do
      {:ok, name} -> name
      {:error, reason} -> raise reason
    end
  end

# If the function is called with an argument that is neither a list
# nor a `%Widget{}`, a runtime match error is raised.
end
	\end{minted}
\end{listing}

Elixir has a powerful macro system, which allows for the arbitrary transformation of the Abstract Syntax Tree (AST) at compile-time~\cite[p.~13]{Elixir-Metaprogramming}.
In fact, most of Elixir's keywords are themselves macros~\cite[p.~21]{Elixir-Metaprogramming}.

Elixir programs often moddel data being transformed by flowing through a pipeline of functions.
To that end, the convention in Elixir for functions which operate on data is to take the value being transformed as the first argument to the function.
The pipeline operator \exs{|>} is used to write expressions like
\begin{minted}[linenos=false]{elixir}
"foo"
|> String.capitalize()
|> String.pad_leading(5)
# produces "  FOO"
\end{minted}
which is equivalent to
\begin{minted}[linenos=false]{elixir}
String.pad_leading(String.capitalize("foo"), 5)		
\end{minted}
but expresses the steps in the computation much more clearly~\cite[p.~56]{Thomas:2016}.
This convention matches the Dataflow Model well, since it too works in terms of defining a pipeline of transformations for data to flow through.

\subsection{Concurrency model and the OTP framework}\label{sec:prep:elixir:otp}

Elixir and Erlang both use \emph{processes}---lightweight user-space threads managed by the VM and scheduled transparently across hardware threads~\cite[p.~185]{Thomas:2016}.
The VM can support many thousands (sometimes millions~\cite{elixir-2million-processes}) of processes with a predictable, linear performance curve (rather than a sudden drop in performance under load), making it very suitable for running highly available concurrent systems.
Processes have private heaps and communicate and synchronise using message-passing.
Data passed in a message is copied between process heaps, isolating their execution~\cite[p.~29]{scalability_erlang_otp}.

The OTP\footnote{
Once `Open Telecom Platform', the initialism now has no further meaning.
} framework ships with the standard Erlang distribution and provides tools to develop robust concurrent and distributed systems.
Processes are organised into supervision trees, with certain processes (\emph{supervisors}) responsible for starting, restarting, and killing child processes.
In this way crashes can be isolated and handled explicitly and locally, resulting in failure-tolerant systems~\cite[p.~169]{scalability_erlang_otp}.

OTP provides powerful concurrency and distribution primitives such as state machines and generic servers which handle many common edge cases and failure modes prevalent in distributed systems.
It is typical to use these primitives to build systems with an actor model.
Isolated processes each handle a single concern and communicate by sending messages.

Only a small portion of the power of OTP is used in the implementation in this project, but it has provided functionality which required thousands of lines of code in the Beam codebase.
A production-ready, distributed implementation of the Model in Elixir would be able to take advantage of the advanced primitives available, greatly reducing code size and complexity and increasing performance over JVM implementations.

\section{The Dataflow Model explained}\label{sec:prep:dataflow}

This section summarises the Model as it was presented originally~\cite{Akidau:2015} in preparation for its extension~(\cref{sec:impl:dataflow}) and implementation~(\cref{sec:impl:approach}) in the following chapter.

The Dataflow Model is a unified model of data processing which can subsume other abstractions, placing them in a well-defined taxonomy and allowing for the efficient execution of disparate computations within one system.

It does this by decoupling the logical description of a computation from its underlying physical implementation.
An abstract data processing model is defined which allows the orthogonal specification of \textbf{what} results are computed, \textbf{where} in event-time they are computed, \textbf{when} in processing-time they are materialised, and \textbf{how} earlier results relate to later refinements~\cite[p.~1793]{Akidau:2015}.

This standard model can then be implemented and realised by many different \emph{runners}. 
Such a runner determines its own execution strategies while producing output consistent with the Model.
This flexible approach allows for the tuning of execution technology to business requirements while working with a consistent, expressive model of the computation itself.

The Model makes as few assumptions as possible about the data inputs, outputs and processes. 
Instead, it provides simple primitives from which complex systems can be constructed.

\subsection{`The What': Transforms and Pipelines}\label{sec:prep:dataflow:what}

In the Dataflow Model, a \emph{Pipeline} is an independent computation.
It may incorporate many different \emph{Transforms} which pass data to one another, transforming it as it flows through them.
One can think of Pipelines as Directed Acyclic Graphs (DAGs), with nodes representing Transforms and edges representing data flowing between them~(\cref{fig:prep:pipeline-dags}).

\begin{figure}
	\subfloat[][A Pipeline can be a simple series of transformations.]{
	\includegraphics[width=0.3\textwidth]{images/diags/pipeline-dag-basic}
	}
	\subfloat[][A complex Pipeline can feature branching and Transforms with multiple inputs and outputs and interact with multiple external systems.]{
	\includegraphics[width=0.7\textwidth]{images/diags/pipeline-dag-complex}
	}
	\caption[Examples of Pipelines, simple and complex, illustrated as Directed Acyclic Graphs.]{Pipelines are expressed as DAGs and can be simple or complex.}
	\label{fig:prep:pipeline-dags}
\end{figure}

A Transform may produce multiple outputs and it may consume multiple inputs.
It can also produce no outputs, instead causing a side effect such as writing to the network, database or file system.
Transforms can also have no inputs in the graph, instead receiving output from an external source or generating it.
These Transforms are called \emph{Root~Transforms}.

An advantage of expressing computation in terms of dataflow is the natural adaptation of the system to \emph{unbounded} data streams---ones which may never finish.
An example of this may be a stream of events from a website, or the Twitter firehose.

It may help to think of unbounded data as streaming data, but `streaming' and 'batch' imply execution strategies as opposed to data properties.
Further, in the Dataflow Model it is more convenient to think of all data as streams, whether they are truly infinite or come from a file or other bounded source.

There is a distinction between constructing the Pipeline by connecting Transforms and describing the computation, and actually executing the Pipeline with real data.
When the Pipeline is executed, the data flowing between Transforms is made up of \emph{elements} which are tuples containing the data itself (often assumed to be key-value pairs), a timestamp, and any windows which have been assigned to the element (described later).

All Transforms can be fundamentally represented using only two primitives: \verb|ParDo| and \verb|GroupByKey|, illustrated in \cref{fig:prep:pardo-gbk}.

\begin{figure}
	\centering
	\includegraphics[width=0.6\textwidth]{images/diags/pardo-gbk}
	\caption[An illustration of the operation of \texttt{ParDo} and \texttt{GroupByKey}.]{\texttt{ParDo} and \texttt{GroupByKey} are used as primitives for the construction of other Transforms. \texttt{ParDo} outputs zero or more elements per element, and \texttt{GroupByKey} outputs precisely one element per key in the input Collection.}
	\label{fig:prep:pardo-gbk}
\end{figure}

\verb|ParDo| represents a flat-map operation---it transforms an input data element into zero or more output data elements.
Its output must depend only on the single input element.
This restriction means that the \verb|ParDo| step is easily parallelisable.
It also translates naturally to operating on unbounded data.

\verb|GroupByKey| groups all key-value pairs into per-key elements containing all values for a particular key.
It needs to collect \emph{all} of the data for a particular key before it is able to emit its output.
An issue arises when dealing with unbounded data---how do we know when we have received all of the input for a particular key?

To deal with this problem, the Dataflow Model uses windowing.

\subsection{`The Where': Windowing}\label{sec:prep:dataflow:where}

\emph{Windowing} is the additional grouping of elements according to their timestamps in a way which enables emission of a chunk of data as soon as all relevant elements have been seen.
In the Dataflow Model, such a chunk of data belonging to a particular window is called a \emph{pane}.

\subsubsection{Time domains}

We distinguish between two time domains when considering our data as events in time: \emph{event-time} and \emph{processing-time}.

\emph{Event-time} describes the time domain in which events occur.
Each element in a Pipeline has an intrinsic timestamp in the event-time domain.
There are two special values---the \emph{minimal} and \emph{maximal timestamps}---representing negative and positive infinity.

Where there is no intrinsic timestamp associated with an element (e.g.\ it is an un-dated record in a file) or its timestamp is not known yet (the record must be processed first), the element receives the minimal timestamp for reasons which will become clear.

\emph{Processing-time} is a property of an executing Pipeline.
It measures real-world time elapsing as the computation progresses.
The processing-time of an element at a Transform is the system clock time at which it was seen by that Transform.

\subsubsection{Windowing in the Dataflow Model}

\begin{figure}
	\subfloat[][Global windowing assigns all elements to a single, infinite window.
It is equivalent to no windowing at all.]{
		\includegraphics[width=0.5\textwidth]{images/diags/window-type-global}
	}
	\subfloat[][Fixed windows have a static window size, e.g.\ hourly, and are \emph{aligned}---each window contains all of the data in the system for that period of time.]{
		\includegraphics[width=0.5\textwidth]{images/temp/window-type-fixed}
	}\\
	\subfloat[][Sliding windows have a static window size and slide period, e.g.\ hourly windows which start every 10 minutes.
The windows may overlap, which requires the system to be able to place a single piece of data in multiple windows.
The windows are still aligned.]{
		\includegraphics[width=0.5\textwidth]{images/temp/window-type-sliding}
	}
	\subfloat[][Sessions are windows which capture some period of activity over a subset of the data.
They are typically specified using a timeout after which a new session starts.
Any events closer in time than that form a session.
These windows need to be able to merge as new data is added between them.
They are also unaligned---certain windows apply only to a subset of the data.]{
		\includegraphics[width=0.5\textwidth]{images/temp/window-type-sessions}
	}
	\caption{Four windowing paradigms, all supported by the Dataflow Model.}
	\label{fig:prep:window-types}
\end{figure}

There are three windowing patterns which are commonly used in practice.
\Cref{fig:prep:window-types} describes and illustrates these, along with global windowing.

A progression in complexity is apparent, and many other systems support only a prefix of this list of paradigms.
The Dataflow Model is able to support all of them by not making assumptions about windows being aligned, non-overlapping or immutable.

An innocation in the Dataflow Model which allows for flexible windowing is the generalisation of what a window is.
The Model requires only that a window is some fixed, comparable value.
In practice, intervals in event-time are used.

The windowing logic is encapsulated in \emph{windowing functions} (\verb|WindowFn|s), each of which comprises two functions: \texttt{assign} and \texttt{merge}.

\texttt{assign} assigns a set of windows to an element based on its timestamp.
If the element is assigned to multiple windows, the element is replicated in each of those windows (since all computation is implicitly grouped by the window).

\texttt{merge} merges some or all of an input set of windows into a new set of windows.
This is used in cases like session windowing~(\cref{fig:prep:sessions-merge}).
Most windowing strategies do not need to merge windows.

\begin{figure}[h]
	\includegraphics[width=\textwidth]{images/temp/sessions-assign-merge}
	\caption[A \texttt{WindowFn} is used to place elements into session-based, unaligned windows.]
	{The sessions \texttt{WindowFn} is used to assign elements over two separate keys into session-based windows. The \texttt{assign} operation puts each element in each key into its own 20 minute-long window. The \texttt{merge} operation takes any overlapping windows and merges them into a larger window spanning all original windows.}
	\label{fig:prep:sessions-merge}
\end{figure}


\subsection{`The When': Watermarks and triggers}\label{sec:prep:dataflow:when}

In the previous section, we have defined a way to group elements into windows.
We have also asserted that we can `emit a chunk of data as soon as all relevant elements have been seen'.
How can this be achieved in light of the fact that data may appear out of order?

The true answer is that it cannot---if we allow unpredictably late data in the Pipeline, then we cannot ever truly be certain that we can emit a complete pane of data.
What we can do, however, is introduce a model which allows us to specify the desired behaviour explicitly.

\subsubsection{Watermarks}

A \emph{watermark} is a lower bound in event-time on the timestamps of values which have been processed by the Pipeline (or a particular Transform therein).
For example, if at a particular instant the watermark is $T$, then we are promised that all events with timestamps earlier than $T$ have been processed.
A graph of watermark progression against processing time (such as the one in \cref{fig:prep:watermark-progression}) is a useful way to visualise the progress of a Pipeline.
Further, this concept allows the construction of a powerful model of lateness in the system~(\cref{sec:impl:dataflow:lateness}).

\begin{figure}
\centering
	\includegraphics[width=0.75\textwidth]{images/diags/watermark-progression-example}
	\caption[An example of a watermark progression plotted against processing-time, showing late data as falling behind the watermark.]{A graph of watermark progression against processing-time shows the progress of a Pipeline. The ideal watermark is an upper bound on progress---processing each element immediately as it is produced. The local watermark is a heuristic for the true watermark. The element marked (a) was late as it arrived behind the watermark.}
	\label{fig:prep:watermark-progression}
\end{figure}

Watermarks in the Dataflow Model are heuristics generated by sources of data, or by user logic in the Pipeline.
This is described further in \cref{sec:impl:dataflow:watermark-generation}.

\subsubsection{Triggering model}

How can a Transform know it has received all input it needs to produce output?
An initial suggestion may be to use the watermark as an indication of when a window is finished and is safe to emit; that is, to emit once the watermark has passed the end of the window.
However, it is possible for the watermark on its own to be \textbf{too fast} or \textbf{too slow}, emitting a pane too early or holding back the entire Pipeline, waiting for data which may never arrive.

To solve these problems, we define a more complex triggering model.

We explicitly allow data in one window to be emitted multiple times.
Each such emission is called a \emph{pane} and is marked as \verb|EARLY|, \verb|ON_TIME| or \verb|LATE|, indicating the timeliness of the data which were used to compute it.

Then, define a \emph{Trigger} as a Finite State Machine (FSM) which has access to timestamps of elements passing through the Transform, as well as watermarks and other timing information.
This FSM can instruct the Transform to emit a pane of currently buffered data at any time.

In this manner, a variety of behaviours is enabled.
For example, one may want to receive approximate results (\verb|EARLY| panes) every ten seconds until the watermark passes the end of the window, at which point an \verb|ON_TIME| pane is emitted.
Any further late data arriving after the watermark can trigger the emission of a \verb|LATE| pane.

\subsection{`The How': Refinements and retractions}\label{sec:prep:dataflow:how}

The Model allows for the selection of a \emph{refinement mode} to be used when emitting multiple panes per window.
Each pane can either contain all of the information seen from the start of the window---the \emph{accumulating mode}, or it can only contain the information seen since the last pane was emitted---the \emph{discarding mode}.

While the Model broadly defines retractions and how they work at a high level, their implementation remains an unsolved problem and is is an area of research in the Beam project~\cite{JIRA-retractions}.
As such, they are out-of-scope for this project.

\section{Goals and requirements}\label{sec:prep:goals}

A side-effect of the way the Dataflow/Beam project has developed is that much of the theory of the Model has been developed implicitly while the software was written.
New concepts were added \emph{in situ}, without a formal definition.
Rather, informal proposals were written and iterated upon in code.

The extraction of these concepts from the Java codebase into a more approachable model formed a large part of the project.
This was further necessitated by the move from OOP to a functional paradigm, requiring each concept to be reduced to an abstract description rather than merely transferring the class hierarchy to a new language.

The initial project plan did not account for this---it was assumed that a reasonable implementation could be produced from the Dataflow paper~\cite{Akidau:2015} using the Beam code as reference.

\section{Software Engineering methods}\label{sec:prep:softeng}

The interleaving of research and engineering required a particular approach to the execution of the project in order to keep the project on-track and enable success.

\subsection{The spiral model}\label{sec:prep:softeng:spiral}

The fluid implementation goals resulted in the need for an adaptive and agile development process, since initial assumptions changed many times over.
Often, core modules had to be re\"implemented as the model grew more complete.

The Elixir language, with its modular structure and a convention of writing simple, composable functions enabled easy refactoring of core code when necessary.

A spiral model~\cite{Boehm:1986} of development was adopted with core modules prototyped, integrated, and then refactored, extended or rewritten as other parts of the system demanded it.
The process is illustrated in \cref{fig:prep:spiral-model}.

Overall, three or four iterations of prototyping and evaluation were performed, going from a basic proof-of-concept to a working, performant system implementing many features of the Model.

\begin{figure}
	\centering
	\includegraphics[width=0.6\textwidth]{images/temp/spiral}
	\caption{The spiral model of development as used in this project.}
	\label{fig:prep:spiral-model}
\end{figure}


\todo{Flesh the diagram out more, indicate the features completed at each prototype iteration.}

\subsection{Source code management}\label{sec:prep:softeng:scm}

The project source code was managed using Git, including a private GitHub upstream repository.

Source control was used to checkpoint and document progress.
This made it easy and safe to experiment and prototype, a very important task in this project.

The dissertation sources were also kept in Git, with all project data synced to Dropbox as well as an external backup service to guard against data loss.

\subsection{Testing approach}\label{sec:prep:softeng:testing}

The modular nature of Elixir makes it easy to write small modules with referentially transparent, easily testable functions.
Small unit tests were written as needed in order to verify correct behaviour of core algorithms.

Additionally, several end-to-end examples were written to exercise the entire system.
Some of these were also re\"implemented in Java and used for comparison purposes in the evaluation phase of the project.

\section{Starting point}\label{sec:prep:starting}

The project was started from scratch---all code was written specifically for this project, during the timeframe of the project.
The author had a good working knowledge of Elixir, having had implemented several projects in it before, but had no experience working with Dataflow or Beam.

The open-source Beam project was used as a reference, but due to fundamental paradigm differences the dissertation project did not simply comprise the transfer of the OOP hierarchy of the system from one language to another.